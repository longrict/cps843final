\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}

\begin{document}

\title{Real-Time Traffic Signal Detection for Live Video Streams Using YOLOv8}

\author{
\IEEEauthorblockN{Longric Tran, Sheel Patel, Hareesh Suresh}
\IEEEauthorblockA{Department of Computer Science\\
Toronto Metropolitan University\\
Toronto, Ontario, Canada\\
longric.tran@torontomu.ca, s60patel@torontomu.ca, hsuresh@torontomu.ca}
}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
This project aims to build a real-time traffic sign detection system by fine-tuning a pre-existing YOLOv8 model. The goal is to detect and classify traffic signs in live video
while keeping latency low enough for practical use. To achieve our goal, a pre-trained YOLOv8 model was retrained on a traffic signs dataset with fifteen  
labels for various signals, and regulatory signs. The system runs through a simple pipeline that includes training, testing, and real 
time detection via webcam using OpenCV. Our results show that YOLOv8 works well for this task, achieving solid detection accuracy while maintaining reasonable
detection speed.

\vspace{0.5em}
\noindent\textbf{Source Code:} \url{https://github.com/longrict/cps843final/}
\end{abstract}

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

Traffic and regulatory signs have the important role of quickly and effectively communicating rules and warnings to drivers across the world. As demand for self-driving cars, 
and driver assistance systems grows, the need for automatic detection and classification systems becomes clear. The main challenges in developing such products however, relates to
balancing the accuracy of the predictions and the speed at which detections can be made. Such systems must also function under different environmental conditions including changes in
lighting, occlusion and weather. 

Many approaches have been created to approach this problem. Many early methods relied heavily on domain knowledge and hand crafted features, but were not robust and would struggle under varying 
environmental conditions. A newer approach through the usage of deep learning and Convolutional Neural Networks (CNN), which learn directly from the data rather than from human intervention, offers
significantly improved robustness and accuracy enabling models to better generalize to varying signs and conditions.

One popular approach uses a CNN-based framework called You Only Look Once (YOLO) for object detection.
Unlike other detectors that look at an image multiple times, YOLO processes the whole image in one pass, significantly improving speeds and enabling its usage for real-time detection.

For this project YOLOv8, a YOLO-style object detection framework developed by Ultralytics, was chosen as a base object detection framework.

The project is composed of three main parts:
1. Model training on a traffic data set.
2. Validation on a test set of still images to measure accuracy.
3. Live tests on a webcam through the usage of OpenCV.

%==============================================================================
% TECHNICAL PART
%==============================================================================
\section{Technical Approach}
\subsection{YOLO Overview}
You Only Look Once (YOLO) is an object detection algorithm introduced in 2015 that classifies objects within a single pass,
and is used to perform the bounding and predictions \cite{viso_yolov8}. It was developed as an improvement over Regions based Convolution 
Neural Networks (R-CNN), which would use regional methods to find areas of interest, run a classifier on each potential bounding area and
then use post-processing to delete duplicates and refine the bounding areas \cite{YOLO_original}.

Modern YOLO algorithm implementations consist of three main components: the \textbf{backbone}, \textbf{neck}, and \textbf{head} \cite{viso_yolov8}. 


\subsubsection{Backbone}
The backbone is the primary feature extraction network and converts raw input data into patterns such as edges, textures and shapes which
are used by other components to perform tasks such as object detection and classification \cite{backbone}.

Typically, they are CNNs trained on large-scale classification datasets and they enable the network to learn many general features \cite{backbone}.
By using a pre-trained backbone, developers significantly improves development as time and resources are not needed to train
the backbone from scratch. 

Although there are many common backbone architectures our base model, developed by Ultralytics, uses Cross Stage Partial Network (CSPNet)
architecture \cite{backbone}. CSPNet first generates a feature map through the usage of convolutional layers, and then splits the map into two groups \cite{CSPNet}.
The first group is processed through dense and tranisition layers while the other group bypasses these layers \cite{CSPNet}. Then, the two groups are re-joined
via concatenation \cite{CSPNet}. By only processing part of the feature maps, CSPNet reduces redudant computation and improves gradient flow \cite{CSPNet}. CSPNet balances
speed and learning capacity.


\subsubsection{Neck}
The neck is the component that connects the backbone to the head \cite{backbone}. The neck processes and combines features from various levels of 
the backbone before predictions can be made in the head \cite{backbone}. Ultralytics uses a Feature Pyramid Network (FPN) which is designed to
detect objects at varying scales/sizes with high precision \cite{backbone}. 

FPN builds a pyramidal structure of feature maps at different resolutions 
and combines them so that each scale has strong semantic information and enough spatial context for accurate detection \cite{FPN}.
First, the image is passed through the backbone to produce feature maps at varying resolutions (Bottom-Up Pathway) \cite{FPN}. As it
is processed, the spatial dimensions decrease while semantic values increases \cite{FPN}. 

Then, the lower resolution layers are upsampled to match the spatial size of the higher resolution, shallower layers (Top-Down Pathway) \cite{FPN}. This allows the layers
to be combined so each position contains both precise spatial detail from the upper layers and rich semantic information from
the lower layers \cite{FPN}.

To merge the layers, element-wise addition is used via Lateral Connection to produce a final set of feature maps that are passed
to the head \cite{FPN}.

\subsubsection{Head}
The primary role of the detection head is to make predictions based on the feature maps generated by the neck \cite{head}.
It aims to predict both the class of the object and its location within the image, often using two separate branches in
modern object detection architectur \cite{head}.

The classification branch, as the name suggests, aims to classify the objects in the image \cite{head}. For each object, a probability/confidence score for each class
is calculated and a prediction is made using the highest scoring class \cite{head}.

The regression branch, however predicts spatial coordinates for the bounding box around the object. It aims to maximize the overlap between the
ground truth bounding box and prediction bounding area \cite{head}. This metric is called Intersection Over Union (IoU) \cite{head}.

\subsection{Dataset Selection}
The Traffic Signs Detection dataset created by P.K. Darabi  was used for this project. 
The dataset is formatted for YOLO, with each image having a corresponding annotation file. 
The annotations follow the standard YOLO format:
\begin{verbatim}
    [class_id] [center_x] [center_y] 
            [width] [height]
\end{verbatim}
The coordinates are normalized (values between 0 and 1), which makes them work regardless of image size. Images in the dataset are 416x416 pixels with 3 color channels.
The dataset contains 15 classes focused on speed limits, traffic lights, and stop signs:
\begin{table}[h]
\centering
\caption{Traffic Sign Classes in Dataset}
\label{tab:categories}
\begin{tabular}{ll}
\toprule
\textbf{Category} & \textbf{Classes} \\
\midrule
Traffic Lights & Green Light, Red Light \\
Speed Limits & 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120 km/h \\
Regulatory & Stop \\
\bottomrule
\end{tabular}
\end{table}
The dataset was split into three sets: 3,530 images for training, 801 images for validation, and a separate test set for final evaluation.
Before training our own model, we tested the default pre trained YOLOv8 model on the dataset to see how it would perform. Since the default model was trained on general objects (the COCO dataset), it returned no detections when given a traffic sign image. This confirmed that we needed to fine tune the model on our specific dataset to get it to recognize traffic signs \cite{kaggle_dataset}.


\subsection{Training Procedure and Configuration}
TODO: describe final configuration after experiments

Starting with a base YOLOv8 model, accuracy was further improved by re-training it with a more traffic-sign-specific dataset outlined in the previous
section. Training was conducted using the Ultralytics YOLO framework built on PyTorch.
Configuration details are summarized in Table \ref{tab:trainingparameters}.

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\label{tab:trainingparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Base Model & YOLOv8 \\
Number of Epochs & 50 \\
Image Input Size & 416 \\
Batch Size & 16 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Real-Time Detection}

TODO: This section needs to be done
%==============================================================================
% EXPERIMENTS
%==============================================================================
\section{Experiments}

\subsection{Training Setup}

During the experimental stage, we tested different model configurations to find the best setup for our traffic sign detection system. We followed a systematic approach, starting with baseline models and then adjusting variables one at a time to understand their impact. We experimented with multiple models adjusting three main variables: the base model size, the image size, and different versions of YOLO. To keep all comparisons fair and consistent, all models were trained for 50 epochs with  majority of  batch sizes bieng 16 using the same dataset split and hardware configuration.

\subsection{Base Model Comparison}

We started by establishing baseline results using three different YOLOv8 model sizes. The nano model (yolov8n) is the smallest variant with approximately 3 million parameters. The small model (yolov8s) contains around 11 million parameters. The medium model (yolov8m) is the largest we tested with roughly 26 million parameters. All three were trained using the standard input resolution of 416x416 pixels.


After training completed, we ran our accuracy test script on each model. The results are shown in Table \ref{tab:baseline}.

\begin{table}[h]
\centering
\caption{Baseline Model Comparison (416px, 50 epochs)}
\label{tab:baseline}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{Inference} \\
\hline
yolov8n & 96.7\% & 87.2\% & 93.0\% & 70ms \\
yolov8s & 95.6\% & 90.8\% & 94.7\% & 142ms \\
yolov8m & 96.0\% & 90.5\% & 95.1\% & 261ms \\
\hline
\end{tabular}
\end{table}

 The resulted showed that the nano baseline achieved 93.0\% mAP@0.5, the small baseline achieved 94.7\%, and the medium baseline achieved 95.1\%. This confirmed that larger model with more trainable parameters produce better accuracy. However, the improvement from nano to medium was only about 2 percentage points, which suggests that even the smallest model captures enough features to perform well on this dataset. This is likely because traffic signs have consistent shapes and colors that do not require extremely deep feature extraction.

We also measured the inference speed of each model. The nano model processed each image approximately 70ms, while the medium model required around 261ms  per image. On our test hardware, this translates to roughly 14 frames per second for the nano model and only 4 frames per second for the medium model. This finding was important because it showed us that the medium model, despite being slightly more accurate, would be too slow for smooth real time video detection.

Based on these results, we concluded that the nano and small models were better suited for our real time application. The medium model's marginal accuracy gain did not justify its significantly slower inference speed.

\subsection{Image Size Experiments}
Now that we have established our baselines, we wanted to see whether increasing the input size (image resolution) would improve accuracy. Our prediction were that larger images preserver more spatial details allowing the model to detect smaller signs that appear far away from the frame. We trained all three models again using 512px input resolution compared to the previous 416px.

\begin{table}[h]
\centering
\caption{Image Size Comparison (512px vs 416px baseline)}
\label{tab:imagesize}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{416px mAP@0.5} & \textbf{512px mAP@0.5} & \textbf{Change} \\
\hline
yolov8n & 93.0\% & 95.3\% & +2.3\% \\
yolov8s & 94.7\% & 96.2\% & +1.5\% \\
yolov8m & 95.1\% & 93.4\% & -1.7\% \\
\hline
\end{tabular}
\end{table}

The results in Table \ref{tab:imagesize} confirmed are hypothesis for smaller models. The nano model improved from 93\% to 95\% mAP@0.5, a gain of 2.3 percentage. The small model improved from 94.7\% to 96.7\%. This increase showed that the additional spatial information gained from the higher resolution images helps the feature extract layers capture finer details. 

We also notices that the yolov8m (medium model)  had some unexpected results going from 95.1\% to 93.4\%, a decrease of 1.5 percentage when increased to a larger image size. After some analysis we presume that the model is overfitting at the higher resolution. The larger model with more parameters may have started to memorize the training example rather than learning the generalized features. 

The detailed results are shown in Table \ref{tab:is512}.
\begin{table}[h]
\centering
\caption{512px Model Results}
\label{tab:is512}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} \\
\hline
yolov8n\_IS512 & 95.6\% & 91.0\% & 95.3\% & 82.9\% \\
yolov8s\_IS512 & 93.8\% & 93.0\% & 96.2\% & 84.3\% \\
yolov8m\_IS512 & 96.3\% & 85.1\% & 93.4\% & 79.8\% \\
\hline
\end{tabular}
\end{table}
From this experiment, we learned that bigger is not always better. The small model with 512 pixel images achieved the best overall result at 96.2\% mAP@0.5, outperforming even the larger medium model. It also achieved the highest mAP@0.5:0.95 at 84.3\%, indicating more precise bounding box predictions.

\subsection{YOLOv11 Comparison}
After experimenting around YOLOV8 configurations, we wanted to also test whether the newer YOLOv11 architecture would perform better. We trained the YOLOv11 small model using the intial 416px image to compare it directly against YOLOv8.

\begin{table}[h]
\centering
\caption{YOLOv8 vs YOLOv11 Comparison}
\label{tab:yolov11}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} \\
\hline
yolov8s\_baseline & 95.6\% & 90.8\% & 94.7\% & 83.1\% \\
yolov11s\_baseline & 96.8\% & 88.1\% & 94.7\% & 83.2\% \\
\hline
\end{tabular}
\end{table}

The results in Table \ref{tab:yolov11} showed that YOLOv11 achieved 94.7\% mAP@0.5, which matched the YOLOv8 small baseline exactly. At first glance, this suggested the two architectures perform equally well on our dataset.

However, when we looked at the individual metrics, we noticed an important difference. YOLOv11 achieved higher precision at 96.8\% compared to 95.6\% for YOLOv8, but lower recall at 88.1\% versus 90.8\%. This told us that YOLOv11 is more conservative with its predictions. It produces fewer false positives but misses more actual signs.

For a traffic sign detection system, we decided that recall is more important than precision. Missing a stop sign or speed limit could have serious safety consequences, while a false detection is just a minor inconvenience. Based on this reasoning, we chose to stick with YOLOv8 for our final system.



\subsection{Per Class Analysis}

After completing are model experiments, we analyzed which sign types were easier and hardest to detect. Table \ref{tab:perclass} shows the per class accuracy for our best performing model (yolov8s\_IS512) based on recall percentage value.

\begin{table}[h]
\centering
\caption{Per Class Accuracy (yolov8s\_IS512)}
\label{tab:perclass}
\begin{tabular}{|l|c|}
\hline
\textbf{Sign Class} & \textbf{mAP\texttt{@}0.5} \\
\hline
Stop & 99.5\% \\
Speed Limit 120 & 98.7\% \\
Speed Limit 30 & 99.1\% \\
Speed Limit 70 & 99.2\% \\
Speed Limit 40 & 98.9\% \\
Speed Limit 20 & 98.9\% \\
Speed Limit 100 & 98.5\% \\
Speed Limit 60 & 98.2\% \\
Speed Limit 50 & 97.6\% \\
Speed Limit 110 & 99.4\% \\
Speed Limit 80 & 95.9\% \\
Speed Limit 90 & 99.5\% \\
Green Light & 83.5\% \\
Red Light & 81.0\% \\
\hline
\end{tabular}
\end{table}

Speed limit signs achieved the highest accuracy across all configurations, with most exceeding 95\% mAP@0.5. The stop sign performed best overall at 99.5\%. We assume this is due to the distinctive octagonal shape and bright red color of stop signs, which create strong visual features that are easy for the model to identify.
Traffic lights proved to be the most challenging category. Green lights achieved only 83.5\% and red lights 81.0\%. We identified several reasons for this lower performance. Traffic lights are often smaller in the frame compared to road signs. Their appearance varies depending on lighting conditions and camera angle. The model also has to distinguish between red and green states, which share the same shape and differ only in which light is illuminated.

\subsection{Result (final configuration}
TODO: Talk about how we have selected the yolov8s model wiht 512 px as are final confuraiton here is the table with all the infromation 

\begin{table}[h]
\centering
\caption{Final Configuration Summary}
\label{tab:final}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Base Model & yolov8s \\
Input Resolution & 512x512 pixels \\
Precision & 93.8\% \\
Recall & 93.0\% \\
mAP@0.5 & 96.2\% \\
mAP@0.5:0.95 & 84.3\% \\
Inference Time & 237ms \\
\hline
\end{tabular}
\end{table}


\subsection{Real Time Performance}

After training, which took some time as the model adjusted and optimized the weights for each class, the test script ran smoothly. The model was able to predict stop signs and other traffic signs with around 70 to 80 percent accuracy. When running real time detection using the laptop's webcam, the system performed smoothly with minimal lag, making it responsive enough for practical use. Testing was done on a standard laptop without a dedicated GPU, which shows that YOLOv8's nano version is lightweight enough for everyday hardware.

\subsection{What Worked and What Didn't}

\textbf{What worked well:}
\begin{itemize}
    \item Regulatory signs (like stop signs) were detected reliably. They have distinctive shapes and colours, and were spotted from various angles.
    \item TODO:add the other things that work. I was only able to test it with a stop sign 
\end{itemize}

\textbf{Challenges we encountered:}
\begin{itemize}
    \item Signs that are very far away (small in the image) are harder to detect.
    \item Signs displayed on a phone screen were difficult to detect. Only the stop sign was recognized reliably, and only when held directly facing the camera.
\end{itemize}

%==============================================================================
% CONCLUSION
%==============================================================================
\section{Conclusion}

TODO:Conclusion need to be added here  


%==============================================================================
% REFERENCES
%==============================================================================
\begin{thebibliography}{00}

\bibitem{viso_yolov8}
Viso.ai, ``YOLOv8: A Complete Guide,'' 2023. [Online]. Available: \url{https://viso.ai/deep-learning/yolov8-guide/}

\bibitem{YOLO_original}
J. Redmon, S. Divvala, R. Girshick, A. Farhadi, ``You Only Look Once: Unified, Real-Time Object Detection'' . [Online] Available: \url{https://arxiv.org/abs/1506.02640}

\bibitem{backbone}
Ultralytics, ``Backbone''. [Online] Available: \url{https://www.ultralytics.com/glossary/backbone}

\bibitem{CSPNet}
C. Wang, H. Liao, I. Yeh, Y. Wu, P. Chen, J. Hsieh, ``CSPNet: A New Backbone that can Enhance Learning Capability of CNN''. [Online] Available: \url{
https://doi.org/10.48550/arXiv.1911.11929
}

\bibitem{FPN}
Ultralytics, ``Feature Pyramid Network (FPN)''. [Online] Available: \url{https://www.ultralytics.com/glossary/feature-pyramid-network-fpn}

\bibitem{head}
Ultralytics, ``Detection Head''. [Online] Available: \url{https://www.ultralytics.com/glossary/detection-head}

\bibitem{kaggle_dataset}
P. K. Darabi, ``Traffic Signs Detection Using YOLOv8,'' Kaggle, 2023. [Online]. Available: \url{https://www.kaggle.com/code/pkdarabi/traffic-signs-detection-using-yolov8}
\end{thebibliography}


\end{document}