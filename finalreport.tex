\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}

\begin{document}

\title{Real-Time Traffic Signal Detection for Live Video Streams Using YOLOv8}

\author{
\IEEEauthorblockN{Longric Tran, Sheel Patel, Hareesh Suresh}
\IEEEauthorblockA{Department of Computer Science\\
Toronto Metropolitan University\\
Toronto, Ontario, Canada\\
longric.tran@torontomu.ca, s60patel@torontomu.ca, hsuresh@torontomu.ca}
}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
This project aims to build a real-time traffic sign detection system by fine-tuning a pre-existing YOLOv8 model. The goal is to detect and classify traffic signs in live video
while keeping latency low enough for practical use. To achieve our goal, a pre-trained YOLOv8 model was retrained on a traffic signs dataset with fifteen  
labels for various signals, and regulatory signs. The system runs through a simple pipeline that includes training, testing, and real 
time detection via webcam using OpenCV. Our results show that YOLOv8 works well for this task, achieving solid detection accuracy while maintaining reasonable
detection speed.

\vspace{0.5em}
\noindent\textbf{Source Code:} \url{https://github.com/longrict/cps843final/} \\
\noindent\textbf{Models:} \url{https://drive.google.com/drive/u/2/folders/12RkMa4FyQbNXTnuyKX-fnqXVLK3Kdh-9}
\end{abstract}

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

Traffic and regulatory signs have the important role of quickly and effectively communicating rules and warnings to drivers across the world. As demand for self-driving cars, 
and driver assistance systems grows, the need for automatic detection and classification systems becomes clear. The main challenges in developing such products however, relates to
balancing the accuracy of the predictions and the speed at which detections can be made. Such systems must also function under different environmental conditions including changes in
lighting, occlusion and weather. 

Many approaches have been created to approach this problem. Many early methods relied heavily on domain knowledge and hand crafted features, but were not robust and would struggle under varying 
environmental conditions. A newer approach through the usage of deep learning and Convolutional Neural Networks (CNN), which learn directly from the data rather than from human intervention, offers
significantly improved robustness and accuracy enabling models to better generalize to varying signs and conditions.

One popular approach uses a CNN-based framework called You Only Look Once (YOLO) for object detection.
Unlike other detectors that look at an image multiple times, YOLO processes the whole image in one pass, significantly improving speeds and enabling its usage for real-time detection.

For this project YOLOv8, a YOLO-style object detection framework developed by Ultralytics, was chosen as a base object detection framework.

The project is composed of three main parts:
1. Model training on a traffic data set.
2. Validation on a test set of still images to measure accuracy.
3. Live tests on a webcam through the usage of OpenCV.

%==============================================================================
% TECHNICAL PART
%==============================================================================
\section{Technical Approach}
\subsection{YOLO Overview}
You Only Look Once (YOLO) is an object detection algorithm introduced in 2015 that classifies objects within a single pass,
and is used to perform the bounding and predictions \cite{viso_yolov8}. It was developed as an improvement over Regions based Convolution 
Neural Networks (R-CNN), which would use regional methods to find areas of interest, run a classifier on each potential bounding area and
then use post-processing to delete duplicates and refine the bounding areas \cite{YOLO_original}.

Modern YOLO algorithm implementations consist of three main components: the \textbf{backbone}, \textbf{neck}, and \textbf{head} \cite{viso_yolov8}. 


\subsubsection{Backbone}
The backbone is the primary feature extraction network and converts raw input data into patterns such as edges, textures and shapes which
are used by other components to perform tasks such as object detection and classification \cite{backbone}.

Typically, they are CNNs trained on large-scale classification datasets and they enable the network to learn many general features \cite{backbone}.
By using a pre-trained backbone, developers significantly improves development as time and resources are not needed to train
the backbone from scratch. 

Although there are many common backbone architectures our base model, developed by Ultralytics, uses Cross Stage Partial Network (CSPNet)
architecture \cite{backbone}. CSPNet first generates a feature map through the usage of convolutional layers, and then splits the map into two groups \cite{CSPNet}.
The first group is processed through dense and tranisition layers while the other group bypasses these layers \cite{CSPNet}. Then, the two groups are re-joined
via concatenation \cite{CSPNet}. By only processing part of the feature maps, CSPNet reduces redudant computation and improves gradient flow \cite{CSPNet}. CSPNet balances
speed and learning capacity.


\subsubsection{Neck}
The neck is the component that connects the backbone to the head \cite{backbone}. The neck processes and combines features from various levels of 
the backbone before predictions can be made in the head \cite{backbone}. Ultralytics uses a Feature Pyramid Network (FPN) which is designed to
detect objects at varying scales/sizes with high precision \cite{backbone}. 

FPN builds a pyramidal structure of feature maps at different resolutions 
and combines them so that each scale has strong semantic information and enough spatial context for accurate detection \cite{FPN}.
First, the image is passed through the backbone to produce feature maps at varying resolutions (Bottom-Up Pathway) \cite{FPN}. As it
is processed, the spatial dimensions decrease while semantic values increases \cite{FPN}. 

Then, the lower resolution layers are upsampled to match the spatial size of the higher resolution, shallower layers (Top-Down Pathway) \cite{FPN}. This allows the layers
to be combined so each position contains both precise spatial detail from the upper layers and rich semantic information from
the lower layers \cite{FPN}.

To merge the layers, element-wise addition is used via Lateral Connection to produce a final set of feature maps that are passed
to the head \cite{FPN}.

\subsubsection{Head}
The primary role of the detection head is to make predictions based on the feature maps generated by the neck \cite{head}.
It aims to predict both the class of the object and its location within the image, often using two separate branches in
modern object detection architectur \cite{head}.

The classification branch, as the name suggests, aims to classify the objects in the image \cite{head}. For each object, a probability/confidence score for each class
is calculated and a prediction is made using the highest scoring class \cite{head}.

The regression branch, however predicts spatial coordinates for the bounding box around the object. It aims to maximize the overlap between the
ground truth bounding box and prediction bounding area \cite{head}. This metric is called Intersection Over Union (IoU) \cite{head}.

\subsection{Dataset Selection}
The Traffic Signs Detection dataset created by P.K. Darabi  was used for this project. 
The dataset is formatted for YOLO, with each image having a corresponding annotation file. 
The annotations follow the standard YOLO format:
\begin{verbatim}
    [class_id] [center_x] [center_y] 
            [width] [height]
\end{verbatim}
The coordinates are normalized (values between 0 and 1), which makes them work regardless of image size. Images in the dataset are 416x416 pixels with 3 color channels.
The dataset contains 15 classes focused on speed limits, traffic lights, and stop signs:
\begin{table}[h]
\centering
\caption{Traffic Sign Classes in Dataset}
\label{tab:categories}
\begin{tabular}{ll}
\toprule
\textbf{Category} & \textbf{Classes} \\
\midrule
Traffic Lights & Green Light, Red Light \\
Speed Limits & 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120 km/h \\
Regulatory & Stop \\
\bottomrule
\end{tabular}
\end{table}
The dataset was split into three sets: 3,530 images for training, 801 images for validation, and a separate test set for final evaluation.
Before training our own model, we tested the default pre trained YOLOv8 model on the dataset to see how it would perform. Since the default model was trained on general objects (the COCO dataset), it returned no detections when given a traffic sign image. This confirmed that we needed to fine tune the model on our specific dataset to get it to recognize traffic signs \cite{kaggle_dataset}.

\subsection{Training Procedure and Configuration}

During the experimental stage, we tested different model configurations to find the best setup for our traffic sign detection system. 
A systematic approach was followed, starting with baseline models and then adjusting variables one at a time to understand their impact. 
We experimented with multiple models adjusting three main variables: the base model size, the image size, and different versions of YOLO. 
To keep all comparisons fair and consistent, all models were trained for 50 epochs and tests were run on the same setup (NVIDIA RTX 3050ti, 4GB VRAM). 
A summary of the different combinations for each model are listed in Table \ref{tab:trainingcombinations}.
\begin{table}[h]
\centering
\caption{Training Combinations}
\label{tab:trainingcombinations}
\begin{tabular}{lllll}
\toprule
\textbf{Model Name} & \textbf{Base Model} & \textbf{Epoch} & \textbf{Input} & \textbf{Batch} \\
\midrule
YOLOv8n & yolov8n\_baseline & 50 & 416 & 16 \\
YOLOv8s & yolov8s\_baseline & 50 & 416 & 16 \\
YOLOv8m & yolov8m\_baseline & 50 & 416 & 16 \\
YOLOv11s & yolov11s\_baseline & 50 & 416 & 16 \\
\midrule
YOLOv8n & yolov8n\_IS512 & 50 & 512 & 16 \\
YOLOv8s & yolov8s\_IS512 & 50 & 512 & 16 \\
YOLOv8m & yolov8m\_IS512 & 50 & 512 & 16 \\
YOLOv11s & yolov11s\_IS512 & 50 & 512 & 16 \\
\midrule
YOLOv8n & yolov8n\_IS320 & 50 & 320 & 16 \\
YOLOv8s & yolov8s\_IS320 & 50 & 320 & 16 \\
YOLOv8m & yolov8m\_IS320 & 50 & 320 & 16 \\
YOLOv11s & yolov11s\_IS320 & 50 & 320 & 16 \\
\midrule
YOLOv8n & yolov8n\_BS8 & 50 & 416 & 8 \\
YOLOv8s & yolov8s\_BS8 & 50 & 416 & 8 \\
YOLOv8m & yolov8m\_BS8 & 50 & 416 & 8 \\
YOLOv11s & yolov11s\_BS8 & 50 & 416 & 8 \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Evaluation Metrics}
To compare model configurations fairly, we evaluated each trained detector using four core metrics reported by the Ultralytics validation script: precision, recall, mean Average Precision (mAP), and inference time.

\textbf{Precision} measures how often a predicted detection is correct:
\[
\text{Precision}=\frac{TP}{TP+FP}.
\]
High precision means the model produces fewer false positives (e.g., drawing a box on a background object that looks like a sign). This matters in real-time systems because frequent false alarms make the overlay unusable and would be unsafe in an ADAS-style application.

\textbf{Recall} measures how many real objects are successfully detected:
\[
\text{Recall}=\frac{TP}{TP+FN}.
\]
High recall is important because missed detections (false negatives) are more costly for traffic infrastructure: failing to detect a stop sign or a red light is worse than producing an occasional extra box.

\textbf{mAP} summarizes detection quality across confidence thresholds using the precision--recall curve. We report \textbf{mAP@0.5} (IoU threshold 0.5), which is a standard ``easier'' criterion for object detection, and is useful for comparing overall detector performance. In general, higher mAP indicates the model is both finding the right objects and placing the bounding boxes in roughly the correct region.

\textbf{Inference time} (reported in milliseconds per image) captures latency, which directly affects real-time usability. Even if a model is slightly more accurate, a large increase in inference time can reduce frame rate and make the webcam demo laggy. Therefore, the final model selection was made by balancing mAP/precision/recall with inference time, rather than optimizing accuracy alone.

\subsection{Results and Real Time Performance}
Using the configuration selected in Table \ref{tab:final}, the final model (YOLOv11s\_IS512) achieved strong overall detection accuracy while keeping inference time low enough for live streaming. Across offline evaluation, the model performed best on speed limit and stop sign categories, while traffic light state detection remained the most challenging class group, consistent with the per-class analysis in Table \ref{tab:map_class} and the confusion matrix in Figure \ref{fig:confusionmatrix}.

For real-time testing, we ran webcam inference with the same trained weights and observed smooth performance with minimal perceived delay. The measured per-frame inference time is on the order of a few milliseconds (Table \ref{tab:imagesizeinference}), and the end-to-end webcam loop remained responsive on our hardware. Figure \ref{fig:pic} shows an example validation batch with predicted boxes and confidence scores, illustrating typical detections produced by the final model.

%==============================================================================
% EXPERIMENTS
%==============================================================================
\section{Experiments}

\subsection{Base Model Comparison}

To have a baseline to evaluate the effectiveness for each hyperparameter change, four baseline models were trained
with a standard configuration included in Table \ref{tab:trainingcombinations}. Each of the four used a different version
of Ultralytic's YOLO model family, with three using various versions of YOLOv8, and the other using YOLOv11.

Specifically, the models YOLOv8n, YOLOv8s and YOLOv8m were chosen from the v8 line for their strong performance
and lower training times compared to the larger models. YOLOv11s was chosen to enable comparison between the older v8 models
and a more modern variant. After training completed, an accuracy test script was run on each model. The results are shown in Table \ref{tab:baseline}.

\begin{table}[h]
\centering
\caption{Baseline Model Comparison (416px, 50 epochs)}
\label{tab:baseline}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{Inference} \\
\hline
yolov8n\_baseline & 87.0\% & 84.9\% & 90.6\% & 3ms \\
yolov8s\_baseline & 90.9\% & 89.9\% & 93.6\% & 5ms \\
yolov8m\_baseline & 95.9\% & 87.6\% & 94.0\% & 10ms \\
yolov11s\_baseline & 93.9\% & 90.6\%& 94.5\% & 5ms \\
\hline
\end{tabular}
\end{table}

As expected, as the model size increases (nano to medium), precision, recall and mAP@0.5 increases while the inference speed decreases due
to the greater computational requirements. Larger models have greater representational capacity, but processing each image
has a greater computational cost with each increase in model size almost doubling the average inference time of its predecessor. 

It should be noted however, that the YOLOv11 small model had comparable results to the YOLOv8 medium model with a significantly
faster inference speed. This can be attributed to YOLOv11 using more modern architecture and techniques to achieve greater results 
at a lower computational cost.

From these, we can conclude that the newer models, such as YOLOv11, are able to achieve comparable or better results than larger
and older models while maintaining faster inference speeds. This suggests that achitectural improvements can offset the need of 
larger models typically are more accurate.

\subsection{Image Size Experiments}
To evaluate the effect of image size on model performance, each baseline model was reconfigured to use both larger (512x512) and
smaller (320x320) input images. It should be noted that the images in the dataset were not altered and remained at (412x512), but
were resized by the training library. The models were then re-tested using the same performance script and the results
are shown in Tables \ref{tab:imagesizeprecision} to \ref{tab:imagesizeinference}.

\begin{table}[h]
\centering
\caption{Image Size Comparison (precision)}
\label{tab:imagesizeprecision}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{320px} & \textbf{512px} & \textbf{Baseline Model Changes} \\
\hline
yolov8n & 83.8\% & 89.5\% & -3.2\%, +2.5 \% \\
yolov8s & 91.9\% & 95.9\% & +1\%, +5\% \\
yolov8m & 94.1\% & 92.8\% & -1.8\%, -3.1\% \\
yolov11s& 92.2\% & 95.4\% & -1.7\%,+1.5\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Image Size Comparison (recall)}
\label{tab:imagesizerecall}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{320px} & \textbf{512px} & \textbf{Baseline Model Change} \\
\hline
yolov8n & 79.9\% & 89.6\% & -5.0\%, +4.7\%\\
yolov8s & 85.8\% & 91.3\% & -4.1\%, +1.4\%\\
yolov8m & 84.7\% & 85.6\% & -2.9\%, -2.0\%\\
yolov11s& 85.8\% & 91.6\% & -4.8\%, +1.0\%\\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Image Size Comparison (mAP@0.5)}
\label{tab:imagesizemAP}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{320px} & \textbf{512px} & \textbf{Baseline Model Change} \\
\hline
yolov8n & 86.1\% & 93.7\% & -4.5\%, +3.1\% \\
yolov8s & 91.2\% & 94.2\% & -2.4\%, +0.6\% \\
yolov8m & 91.7\% & 92.7\% & -2.3\%, -1.3\% \\
yolov11s& 91.7\% & 95.7\% & +1.1\%, +5.1\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Image Size Comparison (inference speed)}
\label{tab:imagesizeinference}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{320px} & \textbf{512px} & \textbf{Baseline Model Change} \\
\hline
yolov8n & 2ms & 3ms &  -1ms, 0ms\\
yolov8s & 3ms & 7ms &  -2ms, +2ms\\
yolov8m & 7ms & 15ms & -3ms, +5ms\\
yolov11s& 3ms & 6ms & -2ms, +1ms\\
\hline
\end{tabular}
\end{table}

Overall image size changes significantly impact a model's performance. In general, a smaller image size 
reduces the model's precision, recall, and mAP@0.5 while increasing inference speed. This occurs because 
smaller images contain less spatial detail and make it more difficult for the model to detect small and distant
objects. Specifically, recall was impacted the most based on the results.

Conversely, larger and higher resolution images contain more spatial detail and preserve more information, but 
require greater resources to process. For the largest model, YOLOv8m, the inference speed increased by nearly 33\%.

It should be noted that in some cases there were unexpected results. In some models the smaller image size increased 
accuracy. This however, is not consistent throughout all models, and some classes may have benefitted from the smaller
resolution via reduced overfitting or more consistent object scales.

From this experiment, it is clear that reducing the input size of the image greatly increases speed but at a cost of
accuracy. Therefore, selecting an appropriate image size requires balancing the trade-off between faster inferencing and 
maintaining acceptable performance.

\subsection{Batch Size}
For each baseline model, a variant with a smaller batch size of 8 was trained. Larger batch size variants (32) were attempted,
however they could not be trained due to the memory limitations of our setup. The performance of each version is shown in 
Tables \ref{tab:bs8precision} to \ref{tab:bs8inference}.

\begin{table}[h]
\centering
\caption{Batch Size 8 Comparison (Precision)}
\label{tab:bs8precision}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{Baseline} & \textbf{BS8} & \textbf{BS8-Baseline Change} \\
\hline
yolov8n  & 87.0\% & 90.4\% & +3.4\% \\
yolov8s  & 90.9\% & 95.6\% & +4.7\% \\
yolov8m  & 95.9\% & 94.6\% & -1.3\% \\
yolov11s & 93.9\% & 93.4\% & -0.5\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Batch Size 8 Comparison (Recall)}
\label{tab:bs8recall}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{Baseline} & \textbf{BS8} & \textbf{BS8-Baseline Change} \\
\hline
yolov8n  & 84.9\% & 85.5\% & +0.6\% \\
yolov8s  & 89.9\% & 86.9\% & -3.0\% \\
yolov8m  & 87.6\% & 90.3\% & +2.7\% \\
yolov11s & 90.6\% & 86.5\% & -4.1\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Batch Size 8 Comparison (mAP@0.5)}
\label{tab:bs8mAP}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{Baseline} & \textbf{BS8} & \textbf{BS8-Baseline Change} \\
\hline
yolov8n  & 90.6\% & 91.2\% & +0.6\% \\
yolov8s  & 93.6\% & 93.6\% & 0.0\% \\
yolov8m  & 94.0\% & 94.5\% & +0.5\% \\
yolov11s & 94.5\% & 93.0\% & -1.5\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Batch Size 8 Comparison (Inference Speed)}
\label{tab:bs8inference}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Base Model} & \textbf{Baseline} & \textbf{BS8} & \textbf{BS8-Baseline Change} \\
\hline
yolov8n  & 2.5ms & 2.44ms & -0.06ms \\
yolov8s  & 4.53ms & 4.45ms & -0.08ms \\
yolov8m  & 10.25ms & 9.93ms & -0.32ms \\
yolov11s & 4.53ms & 4.34ms & -0.19ms \\
\hline
\end{tabular}
\end{table}

Reducing the batch size of the baseline models had varying affects on each of the performance metrics.
YOLOv8n and YOLOv8s saw an increase in precision while YOLOv8m and YOLOv11s saw slight decreases. This suggests that
smaller models benefit from a smaller batch size, likely due to more stable gradient estimates and improved confidence 
calibration during training or evaluation.

Reducing the batch size has decreased recall for most of the models. is is likely due to less stable gradient estimates during 
training, which can reduce the model's ability to consistently detect all relevant objects, particularly smaller or less frequent classes.

In terms of inference speed, changing the batch size has negligible effect. All of the models had very slight decreases in
inference time.


\subsection{Per Class Analysis}
For each model change, performance relative to each object class was analyzed to determine which signs were easiest and 
hardest to detect. The performance of each model in accurating detecting each class is shown in Table \ref{tab:map_class}.
It should be noted that the average for the speed signs is listed instead of each speed sign class due to the size
limitations of this page.

\begin{table}[htbp]
\centering
\caption{Performance (mAP@0.5) of YOLO IS Models Across Classes}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Green Light} & \textbf{Red Light} & \textbf{Speed Signs} & \textbf{Stop} \\
\hline
YOLOv8n\_baseline & 85.0 & 67.9 & 93.0 & 99.5 \\
YOLOv8n\_BS8       & 84.2 & 67.5 & 94.2 & 99.5 \\
YOLOv8n\_IS320     & 73.0 & 63.3 & 85.0 & 99.5 \\
YOLOv8n\_IS512     & 90.8 & 72.2 & 94.2 & 99.5 \\
YOLOv8s\_baseline & 88.4 & 72.9 & 95.2 & 99.5 \\
YOLOv8s\_BS8       & 85.4 & 72.5 & 95.0 & 99.5 \\
YOLOv8s\_IS320     & 78.0 & 68.7 & 93.3 & 99.5 \\
YOLOv8s\_IS512     & 89.9 & 73.3 & 97.1 & 99.5 \\
YOLOv8m\_baseline & 88.5 & 71.6 & 95.0 & 99.5 \\
YOLOv8m\_BS8       & 88.1 & 72.3 & 96.0 & 99.5 \\
YOLOv8m\_IS320     & 78.1 & 67.6 & 93.1 & 99.5 \\
YOLOv8m\_IS512     & 87.7 & 71.6 & 94.2 & 99.5 \\
YOLOv11s\_baseline & 88.9 & 70.2 & 97.0 & 99.5 \\
YOLOv11s\_BS8      & 86.8 & 68.9 & 95.0 & 99.5 \\
YOLOv11s\_IS320    & 79.0 & 66.1 & 94.0 & 99.5 \\
YOLOv11s\_IS512    & 88.9 & 78.6 & 97.0 & 99.5 \\
\hline
\end{tabular}
\label{tab:map_class}
\end{table}

Overall, speed limit signs the highest accuracy across all configurations, with most exceeding 90\% mAP@0.5. The stop sign performed best overall at 99.5\% 
for all models. We assume this is due to the distinctive octagonal shape and bright red color of stop signs, which create strong visual features that are 
easy for the model to identify. 

Traffic lights proved to be the most challenging category. Red lights were the most difficult to detect with an average prediction accuracy of ~70\%.
Green lights were also difficult to detect and had an average prediction accuracy of ~83\%.

We identified several reasons for this lower performance based on the confusion matrix (Figure \ref{fig:confusionmatrix}). Often,
the model would confuse the lights as part of the background rather than as an object. This may be because traffic lights are often smaller in the frame compared to road signs. 
Their appearance varies depending on lighting conditions and camera angle. The model also has to distinguish between red and green states, 
which share the same shape and differ only in which light is illuminated. 

\begin{figure}[h]  
    \centering
    \includegraphics[width=0.8\linewidth]{images/confusion_matrix_normalized.png}  
    \caption{Confusion matrix for YOLOv11s\_IS512}
    \label{fig:confusionmatrix}
\end{figure}


\subsection{Final Configuration}
For the final configuration, YOLOv11 seemed to be the best base model in terms of balancing accuracy and inference time. 
YOLOv11s had comparable accuracy to the largest YOLOv8 model we tested, YOLOv8m, while maintaining an inference
speed similar to YOLOv8s. 

For input image size, we valued accuracy over a slight decrease in inference speed. For our final
configuration, 512x512 was chosen based off of its significant increase in accuracy, with minimal decrease in
inference speed (1ms slower).

In terms of batch size, it was left unchanged from the baseline model (16) as the slight increase in 
inference speed was not worth in trading recall performance. Configuration details of the final model are
summarized in Table \ref{tab:final}, and can be found in the Google Drive link from the abstract.

\begin{table}[h]
\centering
\caption{Final Configuration Summary}
\label{tab:final}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Base Model & YOLOv11s \\
Input Resolution & 512x512 px\\
Name & YOLOv11s\_IS512\\
\hline
\end{tabular}
\end{table}

\subsection{Results and Real Time Performance}
Using the configuration selected in Table \ref{tab:final}, the final model (YOLOv11s\_IS512) achieved strong overall detection accuracy while keeping inference time low enough for live streaming. Across offline evaluation, the model performed best on speed limit and stop sign categories, while traffic light state detection remained the most challenging class group, consistent with the per-class analysis in Table \ref{tab:map_class} and the confusion matrix in Figure \ref{fig:confusionmatrix}.

For real-time testing, we ran webcam inference with the same trained weights and observed smooth performance with minimal perceived delay. The measured per-frame inference time is on the order of a few milliseconds (Table \ref{tab:imagesizeinference}), and the end-to-end webcam loop remained responsive on our hardware. Figure \ref{fig:pic} shows an example validation batch with predicted boxes and confidence scores, illustrating typical detections produced by the final model.


The final model, YOLOv11s\_IS512, was able to predict stop signs and speed signs with greater than 90\% accuracy. 
Traffic lights however remained a greater challenge to accurately detect and predict.
When running real time detection using the laptop's webcam, the system performed smoothly with minimal lag (on average 5ms inference time), 
making it responsive enough for practical use. A collage of images used for validation, classified and showing 
the model's confidence score is shown in Figure \ref{fig:pic}.

\begin{figure}[h]  
    \centering
    \includegraphics[width=0.8\linewidth]{images/val_batch2_pred.jpg}  
    \caption{Validation batch with predictions}
    \label{fig:pic}
\end{figure}


\subsection{What Worked and What Didn't}
\textbf{What worked well:}
\begin{itemize}
    \item \textbf{Stop signs} were detected extremely reliably across configurations (near-perfect mAP@0.5). Their distinctive octagonal shape and high-contrast appearance create strong features for the detector.
    \item \textbf{Speed limit signs} generally achieved high performance (often $>90\%$ mAP@0.5 on average). The circular outline and consistent formatting likely helped generalization.
    \item \textbf{Higher input resolution (512px)} improved performance for smaller objects, increasing recall and mAP for most models at the cost of a modest inference-time increase.
\end{itemize}

\textbf{Challenges we encountered:}
\begin{itemize}
    \item \textbf{Traffic lights} were the hardest category. Red vs. green states share the same geometry and are often small in the frame, making them easier to confuse or miss under glare and exposure changes.
    \item \textbf{Small/far signs} were missed more often due to limited pixel detail, especially at 320px input resolution.
    \item \textbf{Phone-screen testing} was inconsistent (reflections, moir\'e patterns, and brightness differences). Detections were most reliable only when the phone was held close and directly facing the camera.
\end{itemize}


\textbf{Challenges we encountered:}
\begin{itemize}
    \item Signs that are very far away (small in the image) are harder to detect.
    \item Signs displayed on a phone screen were difficult to detect. Only the stop sign was recognized reliably, and only when held directly facing the camera.
\end{itemize}

%==============================================================================
% CONCLUSION
%==============================================================================
\section{Conclusion}

We developed a real-time traffic sign and signal detector by
fine-tuning a pre-trained YOLOv11 model on a 15-class dataset
of speed limits, traffic lights, and stop signs. The resulting
system integrates dataset configuration, training, offline evaluation, and webcam inference in a single pipeline using Ultralytics and OpenCV. The main engineering takeaway is that
transfer learning is essential: a general-purpose COCO model
does not recognize traffic sign categories, but fine-tuning on
a task-specific dataset adapts the detector to the desired label
space. From a deployment perspective, the key requirement is
balancing accuracy with latency; lightweight YOLOv11 variants
can remain responsive on commodity hardware while producing meaningful detections for interactive demonstrations.
Our qualitative analysis highlights the primary limitations of
the current model: distant signs become too small to detect
reliably, and speed-limit digits can be ambiguous under blur or
glare. These observations point to the most promising improvements: collecting more diverse training data, increasing input
resolution when hardware allows, and incorporating temporal
smoothing or a dedicated digit-reading stage for speed limits

\section{Team contributions:} 
\begin{itemize}
    \item Longric Tran: model training, testing outline and implementation
    \item Sheel Patel: dataset selection and configuration, testing analysis
    \item Hareesh Suresh: webcam inference pipeline, testing analysis
\end{itemize}

%==============================================================================
% REFERENCES
%==============================================================================
\begin{thebibliography}{00}

\bibitem{viso_yolov8}
Viso.ai, ``YOLOv8: A Complete Guide,'' 2023. [Online]. Available: \url{https://viso.ai/deep-learning/yolov8-guide/}

\bibitem{YOLO_original}
J. Redmon, S. Divvala, R. Girshick, A. Farhadi, ``You Only Look Once: Unified, Real-Time Object Detection'' . [Online] Available: \url{https://arxiv.org/abs/1506.02640}

\bibitem{backbone}
Ultralytics, ``Backbone''. [Online] Available: \url{https://www.ultralytics.com/glossary/backbone}

\bibitem{CSPNet}
C. Wang, H. Liao, I. Yeh, Y. Wu, P. Chen, J. Hsieh, ``CSPNet: A New Backbone that can Enhance Learning Capability of CNN''. [Online] Available: \url{
https://doi.org/10.48550/arXiv.1911.11929
}

\bibitem{FPN}
Ultralytics, ``Feature Pyramid Network (FPN)''. [Online] Available: \url{https://www.ultralytics.com/glossary/feature-pyramid-network-fpn}

\bibitem{head}
Ultralytics, ``Detection Head''. [Online] Available: \url{https://www.ultralytics.com/glossary/detection-head}

\bibitem{kaggle_dataset}
P. K. Darabi, ``Traffic Signs Detection Using YOLOv8,'' Kaggle, 2023. [Online]. Available: \url{https://www.kaggle.com/code/pkdarabi/traffic-signs-detection-using-yolov8}
\end{thebibliography}


\end{document}
