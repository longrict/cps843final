\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}

\begin{document}

\title{Real-Time Traffic Signal Detection for Live Video Streams Using YOLOv8}

\author{
\IEEEauthorblockN{Longric Tran, Sheel Patel, Hareesh Suresh}
\IEEEauthorblockA{Department of Computer Science\\
Toronto Metropolitan University\\
Toronto, Ontario, Canada\\
longric.tran@torontomu.ca, s60patel@torontomu.ca, hsuresh@torontomu.ca}
}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
This project aims to build a real-time traffic sign detection system by fine-tuning a pre-existing YOLOv8 model. The goal is to detect and classify traffic signs in live video
while keeping latency low enough for practical use. To achieve our goal, a pre-trained YOLOv8 model was retrained on a traffic signs dataset with fifteen  
labels for various signals, and regulatory signs. The system runs through a simple pipeline that includes training, testing, and real 
time detection via webcam using OpenCV. Our results show that YOLOv8 works well for this task, achieving solid detection accuracy while maintaining reasonable
detection speed.

\vspace{0.5em}
\noindent\textbf{Source Code:} \url{https://github.com/longrict/cps843final/}
\end{abstract}

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

Traffic and regulatory signs have the important role of quickly and effectively communicating rules and warnings to drivers across the world. As demand for self-driving cars, 
and driver assistance systems grows, the need for automatic detection and classification systems becomes clear. The main challenges in developing such products however, relates to
balancing the accuracy of the predictions and the speed at which detections can be made. Such systems must also function under different environmental conditions including changes in
lighting, occlusion and weather. 

Many approaches have been created to approach this problem. Many early methods relied heavily on domain knowledge and hand crafted features, but were not robust and would struggle under varying 
environmental conditions. A newer approach through the usage of deep learning and Convolutional Neural Networks (CNN), which learn directly from the data rather than from human intervention, offers
significantly improved robustness and accuracy enabling models to better generalize to varying signs and conditions.

One popular approach uses a CNN-based framework called You Only Look Once (YOLO) for object detection.
Unlike other detectors that look at an image multiple times, YOLO processes the whole image in one pass, significantly improving speeds and enabling its usage for real-time detection.

For this project YOLOv8, a YOLO-style object detection framework developed by Ultralytics, was chosen as a base object detection framework.

The project is composed of three main parts:
1. Model training on a traffic data set.
2. Validation on a test set of still images to measure accuracy.
3. Live tests on a webcam through the usage of OpenCV.

%==============================================================================
% TECHNICAL PART
%==============================================================================
\section{Technical Approach}
\subsection{YOLO Overview}
You Only Look Once (YOLO) is an object detection algorithm introduced in 2015 that classifies objects within a single pass,
and is used to perform the bounding and predictions \cite{viso_yolov8}. It was developed as an improvement over Regions based Convolution 
Neural Networks (R-CNN), which would use regional methods to find areas of interest, run a classifier on each potential bounding area and
then use post-processing to delete duplicates and refine the bounding areas \cite{YOLO_original}.

Modern YOLO algorithm implementations consist of three main components: the \textbf{backbone}, \textbf{neck}, and \textbf{head} \cite{viso_yolov8}. 


\subsubsection{Backbone}
The backbone is the primary feature extraction network and converts raw input data into patterns such as edges, textures and shapes which
are used by other components to perform tasks such as object detection and classification \cite{backbone}.

Typically, they are CNNs trained on large-scale classification datasets and they enable the network to learn many general features \cite{backbone}.
By using a pre-trained backbone, developers significantly improves development as time and resources are not needed to train
the backbone from scratch. 

Although there are many common backbone architectures our base model, developed by Ultralytics, uses Cross Stage Partial Network (CSPNet)
architecture \cite{backbone}. CSPNet first generates a feature map through the usage of convolutional layers, and then splits the map into two groups \cite{CSPNet}.
The first group is processed through dense and tranisition layers while the other group bypasses these layers \cite{CSPNet}. Then, the two groups are re-joined
via concatenation \cite{CSPNet}. By only processing part of the feature maps, CSPNet reduces redudant computation and improves gradient flow \cite{CSPNet}. CSPNet balances
speed and learning capacity.


\subsubsection{Neck}
The neck is the component that connects the backbone to the head \cite{backbone}. The neck processes and combines features from various levels of 
the backbone before predictions can be made in the head \cite{backbone}. Ultralytics uses a Feature Pyramid Network (FPN) which is designed to
detect objects at varying scales/sizes with high precision \cite{backbone}. 

FPN builds a pyramidal structure of feature maps at different resolutions 
and combines them so that each scale has strong semantic information and enough spatial context for accurate detection \cite{FPN}.
First, the image is passed through the backbone to produce feature maps at varying resolutions (Bottom-Up Pathway) \cite{FPN}. As it
is processed, the spatial dimensions decrease while semantic values increases \cite{FPN}. 

Then, the lower resolution layers are upsampled to match the spatial size of the higher resolution, shallower layers (Top-Down Pathway) \cite{FPN}. This allows the layers
to be combined so each position contains both precise spatial detail from the upper layers and rich semantic information from
the lower layers \cite{FPN}.

To merge the layers, element-wise addition is used via Lateral Connection to produce a final set of feature maps that are passed
to the head \cite{FPN}.

\subsubsection{Head}
The primary role of the detection head is to make predictions based on the feature maps generated by the neck \cite{head}.
It aims to predict both the class of the object and its location within the image, often using two separate branches in
modern object detection architectur \cite{head}.

The classification branch, as the name suggests, aims to classify the objects in the image \cite{head}. For each object, a probability/confidence score for each class
is calculated and a prediction is made using the highest scoring class \cite{head}.

The regression branch, however predicts spatial coordinates for the bounding box around the object. It aims to maximize the overlap between the
ground truth bounding box and prediction bounding area \cite{head}. This metric is called Intersection Over Union (IoU) \cite{head}.

\subsection{Dataset Selection}
The Traffic Signs Detection dataset created by P.K. Darabi  was used for this project. 
The dataset is formatted for YOLO, with each image having a corresponding annotation file. 
The annotations follow the standard YOLO format:
\begin{verbatim}
    [class_id] [center_x] [center_y] 
            [width] [height]
\end{verbatim}
The coordinates are normalized (values between 0 and 1), which makes them work regardless of image size. Images in the dataset are 416x416 pixels with 3 color channels.
The dataset contains 15 classes focused on speed limits, traffic lights, and stop signs:
\begin{table}[h]
\centering
\caption{Traffic Sign Classes in Dataset}
\label{tab:categories}
\begin{tabular}{ll}
\toprule
\textbf{Category} & \textbf{Classes} \\
\midrule
Traffic Lights & Green Light, Red Light \\
Speed Limits & 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120 km/h \\
Regulatory & Stop \\
\bottomrule
\end{tabular}
\end{table}
The dataset was split into three sets: 3,530 images for training, 801 images for validation, and a separate test set for final evaluation.
Before training our own model, we tested the default pre trained YOLOv8 model on the dataset to see how it would perform. Since the default model was trained on general objects (the COCO dataset), it returned no detections when given a traffic sign image. This confirmed that we needed to fine tune the model on our specific dataset to get it to recognize traffic signs \cite{kaggle_dataset}.


\subsection{Training Procedure and Configuration}
TODO: describe final configuration after experiments

Starting with a base YOLOv8 model, accuracy was further improved by re-training it with a more traffic-sign-specific dataset outlined in the previous
section. Training was conducted using the Ultralytics YOLO framework built on PyTorch.
Configuration details are summarized in Table \ref{tab:trainingparameters}.

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\label{tab:trainingparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Base Model & YOLOv8 \\
Number of Epochs & 50 \\
Image Input Size & 416 \\
Batch Size & 16 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Real-Time Detection}

TODO: This section needs to be done
%==============================================================================
% EXPERIMENTS
%==============================================================================
\section{Experiments}

\subsection{Training Setup}

TODO: This section needs to be completed

\subsection{Evaluation Metrics}

TODO: Need to look at the code to understand how it works

\subsection{Results}

TODO: What are the result i don't really know what to say here




\subsection{Real Time Performance}

After training, which took some time as the model adjusted and optimized the weights for each class, the test script ran smoothly. The model was able to predict stop signs and other traffic signs with around 70 to 80 percent accuracy. When running real time detection using the laptop's webcam, the system performed smoothly with minimal lag, making it responsive enough for practical use. Testing was done on a standard laptop without a dedicated GPU, which shows that YOLOv8's nano version is lightweight enough for everyday hardware.

\subsection{What Worked and What Didn't}

\textbf{What worked well:}
\begin{itemize}
    \item Regulatory signs (like stop signs) were detected reliably. They have distinctive shapes and colours, and were spotted from various angles.
    \item TODO:add the other things that work. I was only able to test it with a stop sign 
\end{itemize}

\textbf{Challenges we encountered:}
\begin{itemize}
    \item Signs that are very far away (small in the image) are harder to detect.
    \item Signs displayed on a phone screen were difficult to detect. Only the stop sign was recognized reliably, and only when held directly facing the camera.
\end{itemize}

%==============================================================================
% CONCLUSION
%==============================================================================
\section{Conclusion}

TODO:Conclusion need to be added here  


%==============================================================================
% REFERENCES
%==============================================================================
\begin{thebibliography}{00}

\bibitem{viso_yolov8}
Viso.ai, ``YOLOv8: A Complete Guide,'' 2023. [Online]. Available: \url{https://viso.ai/deep-learning/yolov8-guide/}

\bibitem{YOLO_original}
J. Redmon, S. Divvala, R. Girshick, A. Farhadi, ``You Only Look Once: Unified, Real-Time Object Detection'' . [Online] Available: \url{https://arxiv.org/abs/1506.02640}

\bibitem{backbone}
Ultralytics, ``Backbone''. [Online] Available: \url{https://www.ultralytics.com/glossary/backbone}

\bibitem{CSPNet}
C. Wang, H. Liao, I. Yeh, Y. Wu, P. Chen, J. Hsieh, ``CSPNet: A New Backbone that can Enhance Learning Capability of CNN''. [Online] Available: \url{
https://doi.org/10.48550/arXiv.1911.11929
}

\bibitem{FPN}
Ultralytics, ``Feature Pyramid Network (FPN)''. [Online] Available: \url{https://www.ultralytics.com/glossary/feature-pyramid-network-fpn}

\bibitem{head}
Ultralytics, ``Detection Head''. [Online] Available: \url{https://www.ultralytics.com/glossary/detection-head}

\bibitem{kaggle_dataset}
P. K. Darabi, ``Traffic Signs Detection Using YOLOv8,'' Kaggle, 2023. [Online]. Available: \url{https://www.kaggle.com/code/pkdarabi/traffic-signs-detection-using-yolov8}
\end{thebibliography}


\end{document}